---
title: "Analisis Factorial"
output: html_notebook
---

```{r}
library(Hmisc)
library(tidyverse)
library(GGally)
library(boot)
library(psych)
library(ggridges)
```


```{r}
load("../../Pre-procesamiento/dataset_final.rda")

spam <- final_dataset %>% 
  select(A1:P3) %>%
  drop_na()
spam_filtrado <- final_dataset %>% 
  filter(error_otherperception == F & error_selfperception == F) %>%
  select(A1:P3) %>%
  drop_na()
```

Para ver mejor la distribucion de cada variable vamos a utilizar un grafico de ridge lines. 

```{r}
distribucion_todos <- spam %>% pivot_longer(cols = everything(), names_to = 'Var', values_to = 'Res') %>%
  ggplot(
    aes(x = Res, y = Var, fill = Res)
  ) +
  geom_density_ridges(bandwidth = 0.7, fill = 'lightblue' 
    ) + 
  xlab('Respuesta') + ylab('Variable') +
  theme_classic()
distribucion_todos
```

Replicamos el grafico con los datos filtrados

```{r}
distribucion_filtrado <- spam_filtrado %>% pivot_longer(cols = everything(), names_to = 'Var', values_to = 'Res') %>%
  ggplot(
    aes(x = Res, y = Var, fill = Res)
  ) +
  geom_density_ridges(bandwidth = 0.7, fill = 'lightblue'
    ) + 
  xlab('Respuesta') + ylab('Variable') +
  theme_classic()
distribucion_filtrado
```

Las distribuciones son practicamente las mismas. De hecho podemos combinarlas a ambas en un mismo grafico. 

```{r}
ggpubr::ggarrange(distribucion_todos, distribucion_filtrado, nrow = 1, ncol = 2, labels = c('A','B'))
ggsave('uso_dist.png')
```


# Reduccion de la dimensionalidad heuristica

Lo primero que podemos hacer es promediar aquellas variables que creemos que pertenecen a la misma categoria, es decir todas aquellas que son de uso activo y todas aquellas que son de uso pasivo.

```{r}
resumen_actividad_todos <- spam %>% 
  rowwise() %>%
  transmute(
    media_activo = mean(
      c(A1,A2,A3,A4)
      ),
    media_pasivo = mean(
      c(P1,P2,P3)
    ),
    mediana_activo = median(
      c(A1,A2,A3,A4)
    ),
    mediana_pasivo = median(
      c(P1,P2,P3)
    )
  )

resumen_actividad_filtrado <- spam_filtrado %>% 
  rowwise() %>%
  transmute(
    media_activo = mean(
      c(A1,A2,A3,A4)
      ),
    media_pasivo = mean(
      c(P1,P2,P3)
    ),
    mediana_activo = median(
      c(A1,A2,A3,A4)
    ),
    mediana_pasivo = median(
      c(P1,P2,P3)
    )
  )
```

Podemos graficar ahora la relacion entre la media del uso activo y el uso pasivo. Si no hay relacion entre ambas esperariamos ver que no hay ningun tipo de ajuste posibles decir una nube de puntos difusa.

```{r}
scatter_all <- ggplot(
  resumen_actividad_todos,
  aes(x = media_activo, y = media_pasivo)
) +
  geom_jitter(width = 0.025) + 
  geom_smooth(se = F, linetype = 'dashed') + geom_smooth(method = 'lm', color = 'red', se = F) +
  theme_classic() + xlab("Media de uso activo") + ylab("Media de uso pasivo")
scatter_all
```

Se ve una clara relacion entre ambas variables la cual incluso podria ser no lineal teniendo la forma de un polinomio cubico. 

Repetimos el grafico empleando unicamente los datos filtrados

```{r}
scatter_consistent <- ggplot(
  resumen_actividad_filtrado,
  aes(x = media_activo, y = media_pasivo)
) +
  geom_jitter(width = 0.025) + 
  geom_smooth(se = F, linetype = 'dashed') + geom_smooth(method = 'lm', color = 'red', se = F) +
  theme_classic() + xlab("Media de uso activo") + ylab("Media de uso pasivo")
scatter_consistent
```

Sigue habiendo una fuerte relacion lineal.

```{r}
ggpubr::ggarrange(
  scatter_all, scatter_consistent
)
ggsave("scatter_uso.png")
```



# Correlaciones

```{r}
library(corrplot)
```

## Spearman

```{r}
corrplot(cor(spam, method = 'spearman'), method = 'ellipse', type = 'upper', addCoef.col = 'black')
```


```{r}
corrplot(cor(spam_filtrado, method = 'spearman'), method = 'ellipse', type = 'upper', addCoef.col = 'black')
title(main = 'Filtrados', line = 2)
```

## Kendall

```{r}
cor(spam, method = 'kendall') %>% corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```


```{r}
cor(spam_filtrado, method = 'kendall') %>% corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```


## Policlorica

```{r}
library(psych)
polychoric(x = spam)$rho %>%
  corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```

```{r}
polychoric(x = spam_filtrado)$rho %>%
  corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```

## Gamma

Hacer la correlacion gamma no es tan sencillo como con las otras. Para hacerla vamos a usar la funcion `rcorr.cens` del paquete `Hmisc`. El problema con esta funcion es que espera dos vectores de numeros por lo que si le damos el dataframe como argumento (como haciamos en los otros casos) nos va a dar un error. Lo que tenemos que hacer es buscar la manera de decirle a R que calcule la correlacion para todas las combinaciones de las variables. Para eso usamos la funcion `expand.grid`, aplicamos la funcion para calcular la correlacion gamma de manera row-wise, extraemos la columna resultante que contiene la correlacion y convertimos el resultando en una matrix cuadrada segun la cantidad de columnas es decir una matriz 7x7.
Por ultimo le agregamos los nombres de las columnas y las filas.

```{r}
gamma_matrix <- expand.grid(colnames(spam), colnames(spam)) %>% tibble() %>%
  rowwise() %>%
  mutate(
    correlacion = Hmisc::rcorr.cens(spam[[Var1]], spam[[Var2]], outx = TRUE)['Dxy']
  ) %>% 
  pull(correlacion) %>%
  matrix(nrow = ncol(spam), ncol = ncol(spam))
colnames(gamma_matrix) <- colnames(spam)
rownames(gamma_matrix) <- colnames(spam)
```

Ahora si podemos proceder a graficar el resultado.

```{r}
gamma_matrix %>% 
  corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```

```{r}
gamma_matrix_filtrado <- expand.grid(colnames(spam_filtrado), colnames(spam_filtrado)) %>% tibble() %>%
  rowwise() %>%
  mutate(
    correlacion = Hmisc::rcorr.cens(spam_filtrado[[Var1]], spam_filtrado[[Var2]], outx = TRUE)['Dxy']
  ) %>% 
  pull(correlacion) %>%
  matrix(nrow = ncol(spam_filtrado), ncol = ncol(spam_filtrado))
colnames(gamma_matrix) <- colnames(spam_filtrado)
rownames(gamma_matrix) <- colnames(spam_filtrado)
gamma_matrix_filtrado %>% 
corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```

## Uniones Graficos


```{r}
par(mfrow = c(1,2))
corrplot(cor(spam, method = 'spearman'), method = 'ellipse', type = 'upper', addCoef.col = 'black')
corrplot(cor(spam_filtrado, method = 'spearman'), method = 'ellipse', type = 'upper', addCoef.col = 'black')
cor(spam, method = 'kendall') %>% corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
cor(spam_filtrado, method = 'kendall') %>% corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
polychoric(x = spam)$rho %>%
  corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
polychoric(x = spam_filtrado)$rho %>%
  corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
gamma_matrix %>% 
  corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
gamma_matrix_filtrado %>% 
  corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```


# Correlaciones con intervalos

Para comparar los niveles de correlacion de cada una de las variables con los distintos metodos vamos a calcular el nivel de la correlacion y a su vez vamos a calcular los intervalos de confianza del 95% sobre el valor del estadistico correspondiente usando un Bootstrap no parametrico de 5000 muestras.

Para eso primero creamos una funcion que calcula los cuatro tipos de correlacion a partir de dos vectores numericos.

```{r}
all_corr <- function(vector_x, vector_y){
  kendall_cor <- cor(vector_x, vector_y, method = 'kendall') #Correlacion de Kendall
  spearman_cor <- cor(vector_x, vector_y, method = 'spearman') #Correlacion de Spearman
  gamma_cor <- unname(Hmisc::rcorr.cens(vector_x, vector_y, outx = TRUE)['Dxy']) #Correlacion Gamma
  poly_cor <- polychoric(data.frame(a = vector_x, b = vector_y))$rho[2,1] #Correlacion policorica
  c(kendall_cor, spearman_cor, gamma_cor, poly_cor)
}
```

Ahora generamos un datafame con todas las combinaciones de dos variables dadas por nuestras 7 variables de interes.

```{r}
all_comb <- expand.grid(
  colnames(spam), colnames(spam)
)
```

Sobre esa base vamos a aplicarle a cada fila nuestra funcion y convertir los resultados en un dataframe con 3 columnas (una para cada tipo de correlacion) y la misma cantidad de filas que el objeto `all_comb`

```{r}
all_corr_df <- all_comb %>%
  pmap(
    ~ all_corr(spam[[.x]], spam[[.y]])
  ) %>% unlist() %>% matrix(nrow = 49, ncol = 4, byrow = TRUE) %>%
  as.data.frame()
colnames(all_corr_df) <- c('Kendall', 'Spearman', 'Gamma', 'Policorica')
all_corr_df <- all_corr_df %>% mutate(Policorica = round(Policorica, 4))
```

Combinamos ambos dataframes en uno solo uniendo las columnas y eliminamos los valores de 1 donde no hay variabilidad

```{r}
all_comb_corr_df <- cbind(
  all_comb, 
  all_corr_df
) %>%
  filter(Kendall < 1 & Spearman < 1 & Gamma < 1 & Policorica < 1)
```

Calculo los indices de las filas que estan repetidas solo que tienen otro orden. 

```{r}
ind_dup <- str_split(paste(all_comb_corr_df$Var1, all_comb_corr_df$Var2), ' ') %>% 
  lapply(
    str_sort
  ) %>% unlist() %>% 
  matrix(ncol = 2, byrow = TRUE) %>% 
  as.data.frame() %>%
  duplicated()
```

Utilizo los indices calculados anteriormente para filtrar el dataframe 

```{r}
clean_comb_corr <- all_comb_corr_df[ind_dup, ]
```


Para comparar entre los distintos metodos de correlacion vamos a modificar el dataframe `clean_comb_corr` para contener a los pares de variables en una sola columna y las correlaciones vamos a colapsarlas en una sola columna cambiando el dataframe a formato long. Una vez realizadas esas transformaciones podemos graficar poniendo en el eje x el nivel de las distintas correlaciones y en el eje y los pares de variables empleados para calcular la correlacion. 

```{r}
clean_comb_corr %>%
  unite('Var_comb', Var1:Var2, sep = '-') %>%
  pivot_longer(Kendall:Policorica, names_to = 'tipo_cor', values_to = 'corr_val') %>%
  ggplot(
  aes(y = fct_reorder(Var_comb, corr_val), x = corr_val, color = tipo_cor)
) +
  geom_point() + geom_vline(xintercept = 0, linetype = 'dashed') +
  theme_linedraw() +
  theme(legend.position = 'bottom') + xlab('Valor de la correlaci√≥n') + ylab("Pares de variables") +
  scale_color_discrete('Tipo de correlacion')
ggsave('all_cors.png')
```

Con el dataframe ya filtrado vamos a calcular los intervalos para la correlacion gamma utilizando bootstrap. Para esto vamos a usar el paquete `boot` y vamos a generar una nueva funcion llamada `gamma_cor`.

```{r}
gamma_cor <- function(data, indices, variable1, variable2){
  boot_data <- data[indices,]
  vector_x <- boot_data[[variable1]]
  vector_y <- boot_data[[variable2]]
  unname(rcorr.cens(vector_x, vector_y, outx = TRUE)['Dxy'])
}
```

Una vez que tenemos la funcion iteramos por cada par de variables del dataframe para usar la funcion de boot en cada caso y obtenemos como resultado una lista con 21 elementos correspondientes a los resultados de cada 

```{r}
boot_results <- clean_comb_corr %>%
  pmap(
    ~ boot(
        data = spam,
        statistic = gamma_cor,
        R = 5000,
        variable1 = .x,
        variable2 = .y,
        parallel = 'multicore',
        ncpus = 4
      )
  ) 
```

Con los resultados ya calculados podemos construir intervalos de confianza iterando sobre la lista de resultados y aplicando la funcion `boot.ci` en cada iteracion, extrayendo de esta los valores que nos interesan que son los limites del intervalo. 

```{r}
boot_cis <- lapply(
  boot_results,
  function(x) boot.ci(x, type = 'bca')[['bca']][c(4,5)]
)
```

Con esto armamos un dataframe que contenga dos columnas, una para el limite inferior y otra para el limite superior del intervalo. 

```{r}
cis_df <- boot_cis %>% unlist() %>% matrix(ncol = 2, byrow = TRUE) %>%
  as_tibble() %>% rename(low = V1, high = V2) 
```

Unimos el dataframe creado recien con el dataframe que contiene las combinaciones de las variables eliminando los valores de las correlaciones de Kendall y Spearman que no nos interesan siendo que los intervalos corresponden unicamente a la correlacion Gamma.

```{r}
cor_df_combinado <- cbind(
  clean_comb_corr,
  cis_df
) %>% select(!c(Kendall,Spearman))
```

Para tener un adecuado control genero datos nulos a partir de una distribucion aleatoria de las variables de interes. Calculo entonces la metrica de interes para 1000 realizaciones del modelo nulo obteniendo asi una distribucion de la correlacion de interes bajo un modelo nulo.

Para eso generamos una funcion llamada `null_gama_fun` la cual calcula una matriz de correlacion gamma para una realizacion de datos uniformemente distribuidos. 

```{r}
null_gama_fun <- function(){
  matriz <- replicate(7,
          sample(0:5, 281, replace = TRUE)) %>% as.matrix()
  colnames(matriz) <- c(
    paste0('A',1:4),
    paste0('P', 1:3)
  )
  combinaciones <- expand.grid(colnames(matriz), colnames(matriz)) %>%
  tibble()  %>%
  rowwise() %>%
  mutate(
    correlacion = rcorr.cens(matriz[, Var1], matriz[,Var2], outx = TRUE)['Dxy']
  ) %>%
  pull(correlacion) %>%
  matrix(nrow = ncol(matriz), ncol = ncol(matriz))
  colnames(combinaciones) <- colnames(matriz)
  rownames(combinaciones) <- colnames(matriz)
  combinaciones
}
```

Repetimos ese proceso 1000 veces armando un array 3d donde la tercera dimension es el numero de la repeticion. Promediamos cada elemento del array 3d es decir cada correlacion para obtener un estimado final del modelo nulo.

```{r}
null_gama_matriz <- replicate(1000, null_gama_fun()) %>% array(dim = c(7,7,10)) %>%
  rowMeans(dims = 2)
null_gama_matriz %>% corrplot(method = 'ellipse', type = 'upper', addCoef.col = 'black')
```

Vuelvo a replicar un grafico como aquel con todas las correlaciones pero esta vez usando unicamente los valores de las correlaciones de Gamma y los intervalos calculados anteriormente. 

```{r}
cor_df_combinado %>%
  unite('Var_comb', Var1:Var2, sep = '-') %>%
  ggplot(
  aes(y = Var_comb, x = Gamma)
) +
  geom_pointrange(
    aes(xmax = high, xmin = low)
  ) +
  geom_vline(xintercept = 0, linetype = 'dashed') + 
  scale_x_continuous(limits = c(0,1))
```

# CFA

```{r}
library(lavaan)
```
Hacemos un modelo nulo

```{r}
cfa_nulo <- '
uso =~ A1 + A2 + A3 + A4 + P1 + P2 + P3
A1 ~~ A3
'
cfa_nulo <- cfa(cfa_nulo, data = spam, std.lv = TRUE, ordered = TRUE)
summary(cfa_nulo, fit.measures = T)
fitMeasures(cfa_nulo)
```



Primero corremos el modelo usando la estructura teorica postulada.

```{r}
cfa_ActivoPasivo <- 'activo =~ A1 + A2 + A3 + A4
pasivo =~ P1 + P2 + P3
'
cfa_model <- cfa(cfa_ActivoPasivo, data = spam, std.lv = TRUE, ordered = TRUE)
summary(cfa_model, fit.measures = T)
```



```{r}
summary(cfa_model)
fitMeasures(cfa_model)
lavInspect(cfa_model, what = "resid")
resid(cfa_model, type = "standardized")
modificationindices(cfa_model) %>% arrange(desc(mi))
```


```{r}
```

```{r}
cfa_ActivoPasivo_modificado <- 'activo =~ A1 + A2 + A3 + A4
pasivo =~ P1 + P2 + P3
A1 ~~ A3
'
cfa_model_modificado <- cfa(cfa_ActivoPasivo_modificado, data = spam, std.lv = TRUE, ordered = TRUE)
summary(cfa_model_modificado, fit.measures = T)
fitMeasures(cfa_model_modificado)
lavResiduals(cfa_model_modificado)
```




```{r}
performance::model_performance(cfa(cfa_ActivoPasivo, data = spam_filtrado))
residuals(cfa_model)$cov
```


```{r}
parameterEstimates(cfa_model, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select(Item=rhs, Standardized=est, ci.lower, ci.upper, SE=se, Z=z, 'p-value'=pvalue)
```


```{r}
parameterEstimates(cfa_model, standardized=TRUE, rsquare = TRUE) %>% 
  filter(op == "r2") %>% 
  select(Item=rhs, R2 = est) 
```




# PCA

```{r}
princomp(covmat = cor(spam, method = 'spearman')) %>% summary(loadings = T)
princomp(covmat = cor(spam, method = 'spearman')) %>% plot()
princomp(covmat = cov(spam, method = 'spearman')) %>% summary(loadings = T)
princomp(covmat = cov(spam, method = 'kendall')) %>% summary(loadings = T)
princomp(covmat = cor(spam, method = 'kendall')) %>% summary(loadings = T)

princomp(covmat = cor(spam, method = 'spearman'))$sdev^2
princomp(covmat = cor(spam, method = 'kendall'))$sdev^2
princomp(covmat = gamma_matrix) %>% summary(loadings = T)
```




## Policorica

Vamos a utilizar la funcion `principal` del paquete `psych`. Definimos la cantidad de factores (componentes) igual a la cantidad de variables. La rotacion de la matriz la definimos como nula para evitar tener una matriz de componentes correlacionados. Como metodo de correlacion dado que tenemos variables ordinales vamos a utilizar la correlacion polyclorica. 

```{r}
poly_pca <- psych::principal(spam, 7, rotate = 'none', cor = 'poly')
poly_pca$loadings
```

Hacemos un skree plot para ver la proporcion de varianza explicada por cada componente. 

```{r}
ggplot(
  data = tibble(x = 1:7, y = poly_pca$Vaccounted[2,] * 100),
  aes(x = x, y = y)
) +
  geom_col(fill = 'deepskyblue3') +
  geom_point() +
  geom_line() +
  xlab('Componentes') + ylab('Proporcion de la varianza explicada')
```

El primer componente explica el `r round(poly_pca$Vaccounted[2,1] * 100, 2)`% de la varianza y el segundo componente explica el `r round(poly_pca$Vaccounted[2,2] * 100, 2)`% de la varianza. 

```{r}
plot(poly_pca)
fa.parallel(spam %>% as.data.frame(), fm = 'pa', fa = 'pa', cor = 'poly')
```

```{r}
biplot(poly_pca, choose = c(1,2))
```


# CATPCA

```{r}
library(Gifi)
```


```{r}
spam_ordinal <- spam %>% mutate(
  across(.fns = ~factor(.x, levels = 0:5))
) %>% as.data.frame()
```


```{r}
captca <- princals(spam_ordinal)
captca$evals
```

Podemos graficar la transformacion realizada a las variables originales. 

```{r}
plot(captca, plot.type = "transplot")
```
Hacemos un grafico de los loadings de cada variable en los dos componentes extraidos.


```{r}
plot(captca, "loadplot", main = "Loadings Uso Activo y Pasivo")
```

A su vez podemos realizar un scree plot de los diferentes componentes principales. 

```{r}
plot(captca, "screeplot")
```

```{r}
plot(captca$objectscores, xlab = "dim1", ylab = "dim2", col = "RED", cex = .5)
```






```{r}
mgcv::gam(
  fomo ~ s(pasivo, bs = 'cr'), 
  data = data.frame(fomo = final_dataset$fomo_puntaje, pasivo = lavPredict(cfa_model_modificado)[,2])
    ) %>% summary()

lm(
  fomo ~ pasivo + activo,
  data = data.frame(fomo = final_dataset$fomo_puntaje, pasivo = lavPredict(cfa_model_modificado)[,2])
    ) %>% summary()
 
```




```{r}

lm(
  fomo_puntaje ~ ., data = cbind(spam, final_dataset['fomo_puntaje'])
) %>% performance::check_model()

cv <- glmnet::cv.glmnet(
  x = model.matrix(fomo_puntaje~., cbind(spam, final_dataset['fomo_puntaje'])),
  y = final_dataset$fomo_puntaje,
  alpha = 1
)

glmnet::glmnet(
  x = model.matrix(fomo_puntaje~., cbind(spam, final_dataset['fomo_puntaje'])),
  y = final_dataset$fomo_puntaje,
  alpha = 1,
  lambda = cv$lambda.min
) %>% coef()

plot(cv$glmnet.fit, 'lambda')
```


```{r}
lasso <- caret::train(
  fomo_puntaje ~., data = cbind(spam, final_dataset['fomo_puntaje']), 
  method = "glmnet",
  trControl = caret::trainControl("LOOCV"),
  tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(-3, 3, length = 100))
  )


lasso %>% predict(cbind(spam, final_dataset['fomo_puntaje']))
caret::RMSE(
  lasso %>% predict(cbind(spam, final_dataset['fomo_puntaje'])),
  final_dataset$fomo_puntaje
)
```

```{r}
modelo_pls <- caret::train(
  fomo_puntaje ~ .,
  data = cbind(spam, final_dataset['fomo_puntaje']),
  trControl = caret::trainControl("LOOCV"),
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(
    ncomp = seq(1, 5, by = 1)
                         ),
  method = 'pls'
)

modelo_pls
```

```{r}
plot(modelo_pls)
plot(varImp(modelo_pls))
```

